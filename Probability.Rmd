--- 
title: "STA 473 - Probability"
author: "Derek L. Sonderegger"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: dereksonderegger/473
description: ""
---

# Preface {-}

<!--chapter:end:index.Rmd-->


# Introduction to Probability

<!--chapter:end:01_Introduction_to_Probability.Rmd-->


# Conditional Probability

<!--chapter:end:02_Conditional_Probability.Rmd-->


# Random Variables and Distributions

<!--chapter:end:03_Random_Variables.Rmd-->


# Expectations

<!--chapter:end:04_Expectation.Rmd-->


# Common Distributions

<!--chapter:end:05_Common_Distributions.Rmd-->


# (APPENDIX) Appendix {-}
```{r, echo=FALSE}
system("source ~/.bash_profile")
```

# Gamma Function

We want to generalize the factorial function which was 
$$n! = n(n-1)(n-2)\dots(2)(1)$$
for $n \in \{0, 1, 2, \dots\}$ and we define $0!=1$ for notational convenience because that is what we need in many formulas.  Notice that $1! = 1$, $2!=2$ and $3! = 3 * 2!$.

The problem is that this is only defined for the whole numbers (natural numbers and zero).  We want to define a function that allows us to expand this function to the positive real numbers.

Consider the funtion:
$$\Gamma(x) = \int_0^\infty z^{x-1} e^{-z} \,dz$$

First we show that $\Gamma(x) = (x-1)\Gamma(x-1)$ utilizing integration by parts.

$$\begin{aligned}  
u  = z^{x-1}      &\;\;\;  & dv = e^{-z}\,dz \\
du = (x-1)z^{x-2}\,dx &\;\;\;  & v  = -e^{-z} 
\end{aligned}$$

and therefore

$$\begin{aligned}
\Gamma(x) &= \int_0^\infty z^{x-1} e^{-z} \,dz \\
          &= \int_0^\infty u \;dv \\
          &= uv\vert_0^\infty - \int_0^\infty v\, du \\
          &= -z^{x-1}e^{-z}\vert_{z=0}^\infty + \int_0^\infty (x-1)z^{x-2}e^{-z} \, dz\\
          &= (x-1) \int_0^\infty z^{x-2}e^{-z}\,dz\\
          &= (x-1) \Gamma(x-1)
  \end{aligned}$$

Next we notice that $\Gamma(1) = 1 = 0!$ via the following integration
$$\Gamma(1) = \int_0^\infty z^{1-1}e^{-z}\,dz = \int_0^\infty e^{-z} \,dz = -e^{-z} \vert_0^\infty = 1$$

Using these bits, we can see that 
$$\Gamma(2) = 1\cdot \Gamma(1) = 1 = 1!$$
$$\Gamma(3) = 2 \Gamma(2) = 2 = 2!$$
$$\Gamma(4) = 3 \Gamma(3) = 3 \cdot 2! = 3!$$
and for positive integers $n$,
$$\Gamma(n) = (n-1)!$$
which can be re-arranged to see that
$$n! = n \Gamma(n)$$

We conclude by noting that 
$$\Gamma\left( \frac{1}{2} \right) = \sqrt \pi$$ 
_(This result can be obtained by looking at the square of the integral and then doing a trig substitution.)_

A graph of this on the postive real numbers is given below: 

```{r, echo=FALSE, warning=FALSE, message=FALSE, dev='jpeg'}
library(dplyr)
library(ggplot2)
data <- data.frame(x=seq(0,6, by=.005)) %>%
  filter(x > 0) %>%
  mutate( y = gamma(x) )
ggplot(data, aes(x=x, y=y)) +
  geom_line() +
  coord_cartesian(xlim=c(0,5), ylim=c(0,12)) +
  scale_y_continuous(name=expression(Gamma(x)))
```

Finally we note that there is a reasonable approximation (known as Stirling's approximation) to the function for large values of $x$ as
$$\Gamma(x+1) \approx \sqrt{ 2 \pi x } \left( \frac{x}{e} \right)^x$$

# Useful Series Results

## $e^x$
In many calculus books, the following is shown (for any $x$):
$$e^x = \sum_{i=0}^\infty \frac{x^i}{i!}$$

Another series result for $e^x$ is 
$$e^x = \lim_{n \to \infty} \left( 1 + \frac{x_n}{n} \right)^n  \;\;\;\;\; \textrm{ if } \;\; x_n \to x$$
## Geometric Series
The geometric series result is that 
$$\sum_{x=0}^\infty \alpha^x = \frac{1}{1-\alpha}\;\;\;\; \textrm{ if } \;\;\;\vert\alpha\vert<1$$

By repeatedly differentiating both sides we can derive
$$\sum_{x=1}^\infty x \alpha ^{x-1} = \frac{1}{ (1-\alpha)^2 }\;\;\;\; \textrm{ if } \;\;\;\vert\alpha\vert<1$$
and
$$\sum_{x=2}^\infty x(x-1) \alpha^{x-2} = \frac{2}{(1-\alpha)^3}\;\;\;\; \textrm{ if } \;\;\;\vert\alpha\vert<1$$

# Tedious results

## Normal distribution

The normal distribution with parameters $\mu$ and $\sigma^2 > 0$ has pdf
$$f(x) = \frac{1}{\sqrt{2\pi} \sigma} \exp \left[ -\frac{(x-\mu)^2}{2 \sigma^2} \right]$$ 
It is clear $f(x)>0$ for all $x$, but we should confirm that this pdf integrates to one. Unfortunately this integral is one of those that either you know how to solve or you waste a huge amount of time with.

First, we will perform a substitution of $z=\frac{x-\mu}{\sigma}$ and therefore $dz = dx/\sigma$ and we are interested in showing that 
$$\int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-z^2/2} \, dz = 1$$
or equivalently that 
$$\underbrace{\int_{-\infty}^{\infty} e^{-z^2/2} \, dz}_{I} = \sqrt{2\pi}$$
or that
$$I^2 = 2\pi$$
We will consider this $I^2$ value as
$$\begin{aligned} I^2 
  &= \int_{-\infty}^{\infty} e^{-z^2/2} \, dz \; \int_{-\infty}^{\infty} e^{-w^2/2} \, dw \\
  &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-z^2/2} \,e^{-w^2/2} \, dz\,dw \\
  &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{-\frac{1}{2}(z^2 + w^2)} \, dz\,dw \\
  \end{aligned}$$
  
Because $z^2 + w^2$ is the equation of a circle in the $z,w$ plane, we think to convert to polar coordinates with the subsitution $z=r \cos \theta$ and $w= r \sin \theta$ and therefore $z^2 + w^2 = r^2$ and $dz\,dw = r\, dr\, d\theta$ therefore
$$\begin{aligned} I^2 
  &= \int_0^{2\pi} \int_0^{\infty} e^{-\frac{1}{2} r^2} \,r \, dr \, d\theta
  \end{aligned}$$
Finally this integral can be attacked using common methods and we'll do a $u$-substitution with $u=r^2/2$ and therefore $du=r\,dr$ and we have

$$\begin{aligned} I^2 
  &= \int_0^{2\pi} \int_0^{\infty} e^{-u}  \, du \, d\theta \\
  &= \int_0^{2\pi} 1 \, d\theta \\
  &= 2 \pi
  \end{aligned}$$


# Distributions

## Discrete Distributions

+--------------+-------------------------+-------------------------------------------+
|              | Bernoulli(p)            |   Binomial(n, p)                          |
+--------------+-------------------------+-------------------------------------------+
| **p.f.**     | $f(x)=p^x(1-p)^{1-x}$   | $f(x) = {n \choose x} p^x (1-p)^{n-x}$    |
|              | for $x=0,1$             | for $x=0,1,\dots,n$                       |
+--------------+-------------------------+-------------------------------------------+
| **Mean**     | $p$                     | $np$                                      |
+--------------+-------------------------+-------------------------------------------+
| **Variance** | $p(1-p)$                | $np(1-p)$                                 |
+--------------+-------------------------+-------------------------------------------+
| **CDF**      |  $F(x) = e^x$           |                                           |
+--------------+-------------------------+-------------------------------------------+
| **m.g.f.**   | $\psi(t)=pe^t+1-p$      | $\psi(t)=( pe^t + 1-p )^n$                |
+--------------+-------------------------+-------------------------------------------+


'$$F(x) = \cases{ \begin{aligned}
      0       &\;\;\;\;\; x < 0\\
      (1-p)   &\;\;\;\;\; 0 \le x < 1\\
      1       &\;\;\;\;\; 1 \le x 
      \end{aligned}}$$'

```{r}
Bernoulli <- data.frame( 
  name='Bernoulli(p)', 
  pf = '$f(x) = p^x (1-p)^{1-x}$', 
  Mean='$p$',
  Variance = '$p(1-p)$',
  CDF = '$$F(x) = \\cases{ \\begin{aligned} 
    0       & \\;\\;\\;\\;\\; x < 0 \\\\  
    (1-p)   & \\;\\;\\;\\;\\; 0 \\le x < 1 \\\\  
    1       & \\;\\;\\;\\;\\; 1 \\le x  \\end{aligned}}$$',
  mgf = '$\\psi(t) = pe^t + 1-p$' )

Binomial <- data.frame(
  name='Binomial(n,p)',
  pf = '$f(x) = { n \\choose x } p^x (1-p)^{n-x}$',
  Mean = '$np$',
  Variance = '$np(1-p)$',
  CDF = 'Too many cases to be shown here',
  mgf = '$\\psi(t) = ( pe^t - 1-p )^n$'
  )

out <- t( rbind(Bernoulli, Binomial) )
colnames(out) <- out$name
pander::pander(out[-1, ])
```


<!--chapter:end:99_Appendix.Rmd-->

