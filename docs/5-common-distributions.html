<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>STA 474 - Mathematical Statistics</title>
  <meta name="description" content="STA 474 - Mathematical Statistics">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="STA 474 - Mathematical Statistics" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="dereksonderegger/474" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="STA 474 - Mathematical Statistics" />
  
  
  

<meta name="author" content="Derek L. Sonderegger">


<meta name="date" content="2018-04-01">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="4-expectations.html">
<link rel="next" href="6-large-random-samples.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html"><i class="fa fa-check"></i><b>1</b> Introduction to Probability</a><ul>
<li class="chapter" data-level="1.1" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#history-of-probability"><i class="fa fa-check"></i><b>1.1</b> History of Probability</a></li>
<li class="chapter" data-level="1.2" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#interpretations-of-probability"><i class="fa fa-check"></i><b>1.2</b> Interpretations of Probability</a></li>
<li class="chapter" data-level="1.3" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#experiments-and-events"><i class="fa fa-check"></i><b>1.3</b> Experiments and Events</a></li>
<li class="chapter" data-level="1.4" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#review-of-set-theory-ds-1.4"><i class="fa fa-check"></i><b>1.4</b> Review of Set Theory (D&amp;S 1.4)</a></li>
<li class="chapter" data-level="1.5" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#definition-of-probability-ds-1.5"><i class="fa fa-check"></i><b>1.5</b> Definition of Probability (D&amp;S 1.5)</a></li>
<li class="chapter" data-level="1.6" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#finite-sample-spaces-ds-1.6"><i class="fa fa-check"></i><b>1.6</b> Finite Sample Spaces (D&amp;S 1.6)</a></li>
<li class="chapter" data-level="1.7" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#ordered-counting-ds-1.7"><i class="fa fa-check"></i><b>1.7</b> Ordered Counting (D&amp;S 1.7)</a></li>
<li class="chapter" data-level="1.8" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#combinations-ds-1.8"><i class="fa fa-check"></i><b>1.8</b> Combinations (D&amp;S 1.8)</a></li>
<li class="chapter" data-level="1.9" data-path="1-introduction-to-probability.html"><a href="1-introduction-to-probability.html#multinomial-coefficients-ds-1.9"><i class="fa fa-check"></i><b>1.9</b> Multinomial Coefficients (D&amp;S 1.9)</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html"><i class="fa fa-check"></i><b>2</b> Conditional Probability</a><ul>
<li class="chapter" data-level="2.1" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#defining-conditional-probability-ds-2.1"><i class="fa fa-check"></i><b>2.1</b> Defining Conditional Probability (D&amp;S 2.1)</a></li>
<li class="chapter" data-level="2.2" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#independence"><i class="fa fa-check"></i><b>2.2</b> Independence</a></li>
<li class="chapter" data-level="2.3" data-path="2-conditional-probability.html"><a href="2-conditional-probability.html#bayes-theorem"><i class="fa fa-check"></i><b>2.3</b> Bayes’ Theorem</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html"><i class="fa fa-check"></i><b>3</b> Random Variables and Distributions</a><ul>
<li class="chapter" data-level="3.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#defining-random-variables-and-discrete-distributions"><i class="fa fa-check"></i><b>3.1</b> Defining Random Variables and Discrete Distributions</a></li>
<li class="chapter" data-level="3.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#continuous-distributions"><i class="fa fa-check"></i><b>3.2</b> Continuous Distributions</a></li>
<li class="chapter" data-level="3.3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>3.3</b> Cumulative Distribution Function</a></li>
<li class="chapter" data-level="3.4" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-distributions"><i class="fa fa-check"></i><b>3.4</b> Bivariate Distributions</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-discrete"><i class="fa fa-check"></i><b>3.4.1</b> Bivariate Discrete</a></li>
<li class="chapter" data-level="3.4.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#bivariate-continuous"><i class="fa fa-check"></i><b>3.4.2</b> Bivariate Continuous</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#marginal-distributions"><i class="fa fa-check"></i><b>3.5</b> Marginal Distributions</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#discrete-case"><i class="fa fa-check"></i><b>3.5.1</b> Discrete Case</a></li>
<li class="chapter" data-level="3.5.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#continuous-case"><i class="fa fa-check"></i><b>3.5.2</b> Continuous Case</a></li>
<li class="chapter" data-level="3.5.3" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#independence-1"><i class="fa fa-check"></i><b>3.5.3</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#conditional-distributions"><i class="fa fa-check"></i><b>3.6</b> Conditional Distributions</a></li>
<li class="chapter" data-level="3.7" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#functions-of-random-variables"><i class="fa fa-check"></i><b>3.7</b> Functions of Random Variables</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#cdf-method"><i class="fa fa-check"></i><b>3.7.1</b> CDF Method</a></li>
<li class="chapter" data-level="3.7.2" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#pdf-method"><i class="fa fa-check"></i><b>3.7.2</b> pdf Method</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3-random-variables-and-distributions.html"><a href="3-random-variables-and-distributions.html#multivariate-transformations"><i class="fa fa-check"></i><b>3.8</b> Multivariate Transformations</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-expectations.html"><a href="4-expectations.html"><i class="fa fa-check"></i><b>4</b> Expectations</a><ul>
<li class="chapter" data-level="4.1" data-path="4-expectations.html"><a href="4-expectations.html#expectation-of-a-rv"><i class="fa fa-check"></i><b>4.1</b> Expectation of a RV</a></li>
<li class="chapter" data-level="4.2" data-path="4-expectations.html"><a href="4-expectations.html#properties-of-expectations"><i class="fa fa-check"></i><b>4.2</b> Properties of Expectations</a></li>
<li class="chapter" data-level="4.3" data-path="4-expectations.html"><a href="4-expectations.html#variance"><i class="fa fa-check"></i><b>4.3</b> Variance</a></li>
<li class="chapter" data-level="4.4" data-path="4-expectations.html"><a href="4-expectations.html#moments-and-moment-generating-functions"><i class="fa fa-check"></i><b>4.4</b> Moments and Moment Generating Functions</a></li>
<li class="chapter" data-level="4.5" data-path="4-expectations.html"><a href="4-expectations.html#mean-vs-median"><i class="fa fa-check"></i><b>4.5</b> Mean vs Median</a></li>
<li class="chapter" data-level="4.6" data-path="4-expectations.html"><a href="4-expectations.html#covariance-and-correlation"><i class="fa fa-check"></i><b>4.6</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="4.7" data-path="4-expectations.html"><a href="4-expectations.html#conditional-expectation"><i class="fa fa-check"></i><b>4.7</b> Conditional Expectation</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-common-distributions.html"><a href="5-common-distributions.html"><i class="fa fa-check"></i><b>5</b> Common Distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="5-common-distributions.html"><a href="5-common-distributions.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="5-common-distributions.html"><a href="5-common-distributions.html#bernoulli-and-binomial"><i class="fa fa-check"></i><b>5.2</b> Bernoulli and Binomial</a></li>
<li class="chapter" data-level="5.3" data-path="5-common-distributions.html"><a href="5-common-distributions.html#hypergeometric"><i class="fa fa-check"></i><b>5.3</b> Hypergeometric</a></li>
<li class="chapter" data-level="5.4" data-path="5-common-distributions.html"><a href="5-common-distributions.html#poisson"><i class="fa fa-check"></i><b>5.4</b> Poisson</a></li>
<li class="chapter" data-level="5.5" data-path="5-common-distributions.html"><a href="5-common-distributions.html#geometric-and-negative-binomial"><i class="fa fa-check"></i><b>5.5</b> Geometric and Negative Binomial</a></li>
<li class="chapter" data-level="5.6" data-path="5-common-distributions.html"><a href="5-common-distributions.html#normal"><i class="fa fa-check"></i><b>5.6</b> Normal</a></li>
<li class="chapter" data-level="5.7" data-path="5-common-distributions.html"><a href="5-common-distributions.html#exponential-and-gamma"><i class="fa fa-check"></i><b>5.7</b> Exponential and Gamma</a></li>
<li class="chapter" data-level="5.8" data-path="5-common-distributions.html"><a href="5-common-distributions.html#beta"><i class="fa fa-check"></i><b>5.8</b> Beta</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-large-random-samples.html"><a href="6-large-random-samples.html"><i class="fa fa-check"></i><b>6</b> Large Random Samples</a><ul>
<li class="chapter" data-level="6.1" data-path="6-large-random-samples.html"><a href="6-large-random-samples.html#introduction-1"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="6-large-random-samples.html"><a href="6-large-random-samples.html#law-of-large-numbers"><i class="fa fa-check"></i><b>6.2</b> Law of Large Numbers</a></li>
<li class="chapter" data-level="6.3" data-path="6-large-random-samples.html"><a href="6-large-random-samples.html#central-limit-theorem"><i class="fa fa-check"></i><b>6.3</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-estimation.html"><a href="7-estimation.html"><i class="fa fa-check"></i><b>7</b> Estimation</a><ul>
<li class="chapter" data-level="7.1" data-path="7-estimation.html"><a href="7-estimation.html#statistical-inference"><i class="fa fa-check"></i><b>7.1</b> Statistical Inference</a></li>
<li class="chapter" data-level="7.2" data-path="7-estimation.html"><a href="7-estimation.html#prior-and-posterior-distributions"><i class="fa fa-check"></i><b>7.2</b> Prior and Posterior Distributions</a></li>
<li class="chapter" data-level="7.3" data-path="7-estimation.html"><a href="7-estimation.html#conjugate-prior-distributions"><i class="fa fa-check"></i><b>7.3</b> Conjugate Prior Distributions</a></li>
<li class="chapter" data-level="7.4" data-path="7-estimation.html"><a href="7-estimation.html#bayes-estimators"><i class="fa fa-check"></i><b>7.4</b> Bayes Estimators</a></li>
<li class="chapter" data-level="7.5" data-path="7-estimation.html"><a href="7-estimation.html#maximum-likelihood-estimators"><i class="fa fa-check"></i><b>7.5</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="7.6" data-path="7-estimation.html"><a href="7-estimation.html#properties-of-maximum-likelihood-estimators"><i class="fa fa-check"></i><b>7.6</b> Properties of Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="7.7" data-path="7-estimation.html"><a href="7-estimation.html#sufficient-statistics"><i class="fa fa-check"></i><b>7.7</b> Sufficient Statistics</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html"><i class="fa fa-check"></i><b>8</b> Sampling Distributions of Estimators</a><ul>
<li class="chapter" data-level="8.1" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#sampling-distribution-of-a-statistic"><i class="fa fa-check"></i><b>8.1</b> Sampling Distribution of a Statistic</a></li>
<li class="chapter" data-level="8.2" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#the-chi-squared-distribution"><i class="fa fa-check"></i><b>8.2</b> The Chi-Squared Distribution</a></li>
<li class="chapter" data-level="8.3" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#joint-distribution-of-the-sample-mean-and-sample-variance"><i class="fa fa-check"></i><b>8.3</b> Joint Distribution of the Sample Mean and Sample Variance</a></li>
<li class="chapter" data-level="8.4" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#t-distribution"><i class="fa fa-check"></i><b>8.4</b> t-distribution</a></li>
<li class="chapter" data-level="8.5" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#confidence-intervals"><i class="fa fa-check"></i><b>8.5</b> Confidence Intervals</a></li>
<li class="chapter" data-level="8.6" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#bayesian-analysis-of-data-from-normal-distribution"><i class="fa fa-check"></i><b>8.6</b> Bayesian Analysis of Data from Normal Distribution</a></li>
<li class="chapter" data-level="8.7" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#unbiased-estimators"><i class="fa fa-check"></i><b>8.7</b> Unbiased Estimators</a></li>
<li class="chapter" data-level="8.8" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#fishers-information"><i class="fa fa-check"></i><b>8.8</b> Fisher’s Information</a></li>
<li class="chapter" data-level="8.9" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#bayes-estimators-1"><i class="fa fa-check"></i><b>8.9</b> Bayes Estimators</a></li>
<li class="chapter" data-level="8.10" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#maximum-likelihood-estimators-1"><i class="fa fa-check"></i><b>8.10</b> Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="8.11" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#properties-of-maximum-likelihood-estimators-1"><i class="fa fa-check"></i><b>8.11</b> Properties of Maximum Likelihood Estimators</a></li>
<li class="chapter" data-level="8.12" data-path="8-sampling-distributions-of-estimators.html"><a href="8-sampling-distributions-of-estimators.html#sufficient-statistics-1"><i class="fa fa-check"></i><b>8.12</b> Sufficient Statistics</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="A-gamma-function.html"><a href="A-gamma-function.html"><i class="fa fa-check"></i><b>A</b> Gamma Function</a></li>
<li class="chapter" data-level="B" data-path="B-useful-series-results.html"><a href="B-useful-series-results.html"><i class="fa fa-check"></i><b>B</b> Useful Series Results</a><ul>
<li class="chapter" data-level="B.1" data-path="B-useful-series-results.html"><a href="B-useful-series-results.html#ex"><i class="fa fa-check"></i><b>B.1</b> <span class="math inline">\(e^x\)</span></a></li>
<li class="chapter" data-level="B.2" data-path="B-useful-series-results.html"><a href="B-useful-series-results.html#geometric-series"><i class="fa fa-check"></i><b>B.2</b> Geometric Series</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="C-tedious-results.html"><a href="C-tedious-results.html"><i class="fa fa-check"></i><b>C</b> Tedious results</a><ul>
<li class="chapter" data-level="C.1" data-path="C-tedious-results.html"><a href="C-tedious-results.html#normal-distribution"><i class="fa fa-check"></i><b>C.1</b> Normal distribution</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STA 474 - Mathematical Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="common-distributions" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Common Distributions</h1>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<p>No Problems.</p>
</div>
<div id="bernoulli-and-binomial" class="section level2">
<h2><span class="header-section-number">5.2</span> Bernoulli and Binomial</h2>

<div class="definition">
<span id="def:unnamed-chunk-23" class="definition"><strong>Definition 5.1  </strong></span>Define a Bernoulli random variable as a variable that takes on the value 0 with probability <span class="math inline">\(1-p\)</span> and the value 1 with probability <span class="math inline">\(p\)</span> where <span class="math inline">\(0\le p \le 1\)</span>. We will write this as <span class="math inline">\(X\sim Bernoulli( p )\)</span>.
</div>
<p> The pf of a Bernoulli random variable is written as <span class="math display">\[f(x) = p^x (1-p)^{1-x} \;\; \cdot I(x \in \{0,1\})\]</span></p>
<p>One convenient place for calculating probabilities is a Web App created by Chester Ismay: <a href="https://ismay.shinyapps.io/ProbApp/" class="uri">https://ismay.shinyapps.io/ProbApp/</a></p>
<p>Alternatively you could use Mathematica, Wolfram Alpha, or R. In a pinch, you could resort to the tables in the back of your book, but I wouldn’t recommend that.</p>
<ol style="list-style-type: decimal">
<li>Show that if <span class="math inline">\(X\sim Bernoulli(p)\)</span> then
<ol style="list-style-type: lower-alpha">
<li>The expectation of <span class="math inline">\(X\)</span> is <span class="math inline">\(E(X) = p\)</span></li>
<li>The variance of <span class="math inline">\(X\)</span> is <span class="math inline">\(Var(X) = p(1-p)\)</span></li>
<li>The moment generating function of <span class="math inline">\(X\)</span> is <span class="math inline">\(\psi(t)=pe^t + (1-p)\)</span></li>
</ol></li>
</ol>

<div class="definition">
<span id="def:unnamed-chunk-24" class="definition"><strong>Definition 5.2  </strong></span>Define a Binomial random variable as the sum of <span class="math inline">\(n\)</span> independent and identically distributed Bernoulli(p) random variables. We will write <span class="math inline">\(X \sim Bin(n,p)\)</span>
</div>
<p> The pf of a Binomial random variable is <span class="math display">\[f(x)= {n \choose x}p^x (1-p)^{n-x} \;\;\cdot I(x\in \{0,1,2,\dots,n\})\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Show that if <span class="math inline">\(X \sim Bin(n,p)\)</span> then
<ol style="list-style-type: lower-alpha">
<li>The expectation of <span class="math inline">\(X\)</span> is <span class="math inline">\(E(X) = np\)</span></li>
<li>The variance of <span class="math inline">\(X\)</span> is <span class="math inline">\(Var(X) = np(1-p)\)</span></li>
<li>The moment generating function of <span class="math inline">\(X\)</span> is <span class="math inline">\(\left( pe^t + (1-p) \right)^n\)</span></li>
</ol></li>
<li><p>Suppose that a fair coin (<span class="math inline">\(p=0.5\)</span>) is tossed independently four times. What is the probability that the number of heads is greater than the number of tails? What if we tossed the coin ten times?</p></li>
<li><p>Suppose that we have an airplane that seats 200 passengers. Through long experience we know that approximately 1% of customers don’t show up to their flight, So to maximize our profit, we might sell 202 tickets and if 2 people don’t show up, then we make more money. Given that we have sold 202 tickets, what is the probability that everybody who shows up for the flight has a seat?</p></li>
<li><p>My two children are throwing toys into a toy chest. Elise has three toys and each has probability of <span class="math inline">\(2/3\)</span> of going into the chest. Casey has four toys to throw but his probability is only <span class="math inline">\(1/3\)</span>. What is the expected number of toys to make it into the chest?</p></li>
</ol>
</div>
<div id="hypergeometric" class="section level2">
<h2><span class="header-section-number">5.3</span> Hypergeometric</h2>
<p>We next consider a distribution that is the sum of <em>dependent</em> Bernoulli random variables. Consider the scenario where there are <span class="math inline">\(A\)</span> balls that are <em>amber</em> and <span class="math inline">\(B\)</span> balls that are <em>blue</em>. The balls are thoroughly mixed and we will select <span class="math inline">\(n\)</span> of those <em>without replacement</em>. Of interest is <span class="math inline">\(Y\)</span>, the number of amber balls drawn. It is helpful to think of randomly arranging all <span class="math inline">\(A+B\)</span> balls into some order and then selecting the first <span class="math inline">\(n\)</span> balls. Define <span class="math inline">\(X_i=1\)</span> if the <span class="math inline">\(i\)</span>th ball is amber and <span class="math inline">\(X_i=0\)</span> if the <span class="math inline">\(i\)</span>th ball is blue. Finally we note that <span class="math inline">\(X_i\)</span> is <em>not</em> independent of <span class="math inline">\(X_j\)</span> and that <span class="math display">\[Y=\sum_{i=1}^n X_i\]</span></p>
<ol style="list-style-type: decimal">
<li>Derive the pf of <span class="math inline">\(Y \sim Hypergeometric(A,B,n)\)</span>.
<ol style="list-style-type: lower-alpha">
<li>How many ways are there to draw, without replacement, <span class="math inline">\(n\)</span> balls from <span class="math inline">\(A+B\)</span> when order doesn’t matter?</li>
<li>How many ways are there to draw <span class="math inline">\(x\)</span> amber balls and <span class="math inline">\(n-x\)</span> blue balls (assuming <span class="math inline">\(x\le A\)</span> and <span class="math inline">\(n-x \le B\)</span>)?</li>
<li>Give the pf of a Hypergeometric(A,B,n) distribution.</li>
</ol></li>
<li><p>Notice that absent any information about the other <span class="math inline">\(X_j\)</span> balls, <span class="math inline">\(X_i \sim Bernoulli \Big( \frac{A}{A+B} \Big)\)</span>. Use this information to derive the expectation of <span class="math inline">\(Y\)</span>.</p></li>
<li>Because <span class="math inline">\(X_i\)</span> is negatively correlated with <span class="math inline">\(X_j\)</span>, we can’t easily derive the variance of <span class="math inline">\(Y\)</span>. Instead we will be <em>obnoxiously</em> clever.
<ol style="list-style-type: lower-alpha">
<li>Let <span class="math inline">\(n=A+B\)</span> so that we are selecting all the balls. Argue that <span class="math inline">\(Var(Y) = 0\)</span>.</li>
<li>Again with <span class="math inline">\(n=A+B\)</span>, notice that because of the symmetry of the random assignments of balls, then <span class="math inline">\(Cov(X_i,X_j)\)</span> is the same for all <span class="math inline">\(i\ne j\)</span>. <em>There isn’t anything thing to do here, but this part of our arguement is critical.</em></li>
<li>We know that for any set of random variables <span class="math display">\[Var(Y) = \sum_{i=1}^n Var(X_i) + \sum_{i \ne j} Cov(X_i,X_j)\]</span> Use this information, along with parts (a) and (b), to solve for the <span class="math inline">\(Cov(X_i, X_j)\)</span> in terms of just <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> under the <span class="math inline">\(n=A+B\)</span> condition. <em>The hard part here is figuring out how many covariance terms there are. Because this result is just a statement about two draws, then just as in problem 2, it doesn’t matter how many balls are drawn. This step gives us the covariance between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> regardless of the number draws.</em></li>
<li>Finally, letting <span class="math inline">\(n\)</span> be an arbitrary integer between <span class="math inline">\(0\)</span> and <span class="math inline">\(A+B\)</span>, and the <span class="math inline">\(Cov(X_i, X_j)\)</span> calculated above, show that <span class="math display">\[Var(Y) = \frac{nAB}{(A+B)^2}\cdot \frac{A+B-n}{A+B-1}\]</span></li>
</ol></li>
<li>My daughter recently mixed <span class="math inline">\(20\)</span> M&amp;Ms and <span class="math inline">\(30\)</span> Skittles in a bowl and left them for me to find.
<ol style="list-style-type: lower-alpha">
<li>What is the probability that I select <strong>only</strong> M&amp;Ms when I select 6 pieces of candy?</li>
<li>What is the expected number of M&amp;Ms in the next 6 pieces (from the 50)?</li>
<li>What kind of monster am I raising?</li>
</ol></li>
</ol>
</div>
<div id="poisson" class="section level2">
<h2><span class="header-section-number">5.4</span> Poisson</h2>
<p>The Poisson distribution is used to model the number of events that happen in some unit time. The critical idea is that the number of events that occur in any two disjoint time periods are independent, regardless of the length of the period. By splitting the time unit into <em>many</em> sub-intervals, each of which that could only have 0 or 1 event and considering those sub-intervals as independent Bernoulli RVs, it is possible to justify the following probability function <span class="math display">\[f(x) = \frac{e^{-\lambda} \lambda^x}{x!} \;\;\cdot I(x \in \{0,1,2,\dots\} )\]</span> where the parameter <span class="math inline">\(\lambda\)</span> represents the mean number of events that should happen per time interval of some specific size.</p>
<ol style="list-style-type: decimal">
<li>Suppose that <span class="math inline">\(X\sim Poisson(\lambda)\)</span>. Show that
<ol style="list-style-type: lower-alpha">
<li>This is a valid pf by showing that <span class="math inline">\(f(x) \ge 0\)</span> for all <span class="math inline">\(x\)</span> and that <span class="math inline">\(\sum_{x=0}^\infty f(x) = 1\)</span>. <em>Hint, look at the series expansion of <span class="math inline">\(e^\lambda\)</span></em>.</li>
<li>The expectation of <span class="math inline">\(X\)</span> is <span class="math inline">\(E(X)=\lambda\)</span>.</li>
<li>The variance of <span class="math inline">\(X\)</span> is <span class="math inline">\(Var(X)=\lambda\)</span>. <em>Hint, derive <span class="math inline">\(E[ X(X-1) ]\)</span> and use that to figure out <span class="math inline">\(E[X^2]\)</span>.</em></li>
<li>The moment generating function of <span class="math inline">\(X\)</span> is <span class="math display">\[\psi(t)=e^{\lambda(e^t-1)}\]</span></li>
</ol></li>
<li><p>Show that if <span class="math inline">\(X_1,\dots,X_n\)</span> are independent and identically distributed <span class="math inline">\(Poisson(\lambda)\)</span> random variables, which we denote as <span class="math display">\[X_i \stackrel{iid}{\sim} Poisson(\lambda)\]</span> then <span class="math display">\[Y=\sum(X_i) \sim Poisson(n\lambda).\]</span></p></li>
<li>While driving along the interstate, the number of cars that pass us is reasonably approximated by a Poisson process with rate parameter <span class="math inline">\(\lambda=0.3\)</span> cars per minute.
<ol style="list-style-type: lower-alpha">
<li>What is the rate parameter if we wanted it in cars per hour?</li>
<li>What is the probability that in a 5 minute stretch of time, that 2 cars pass us?</li>
<li>What is the probability that in a 5 minute stretch of time, that 4 or more cars pass us?</li>
</ol></li>
<li>Suppose that the number of items produced by a machine follows a Poisson distribution with mean <span class="math inline">\(\lambda\)</span>. Each of those items independently has a probability of being defective of <span class="math inline">\(p\)</span>. We are interested in the marginal distribution the total number defective items produced. Let <span class="math inline">\(Y\)</span> be the number of defective items, <span class="math inline">\(N\)</span> be the number of items produced, and <span class="math inline">\(X_i\)</span> be whether the <span class="math inline">\(i\)</span>th item produced is defective.
<ol style="list-style-type: lower-alpha">
<li>What is the distribution of <span class="math inline">\(X_i\)</span>? What is the distribution of <span class="math inline">\(N\)</span>? What is the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(N\)</span>?</li>
<li>What is the joint distribution of <span class="math inline">\(Y\)</span> and <span class="math inline">\(N\)</span>?</li>
<li>By summing out <span class="math inline">\(N\)</span>, what is the marginal distribution of <span class="math inline">\(Y\)</span>? <em>Hint: Notice that <span class="math inline">\(Y\le N\)</span></em>.</li>
</ol></li>
</ol>
</div>
<div id="geometric-and-negative-binomial" class="section level2">
<h2><span class="header-section-number">5.5</span> Geometric and Negative Binomial</h2>
<p>We will first define the <em>geometric</em> distribution. Here we consider another version of multiple Bernoulli random variables. This time, we consider an experiment where we repeatedly sample from a <span class="math inline">\(Bernoulli(p)\)</span> distribution, where <span class="math inline">\(p\)</span> is the probability the draw was a success, and each draw is independent of all previous draws. We are interested in <em>“the number of failures before the first success.”</em></p>
<ol style="list-style-type: decimal">
<li><p>Show that the Moment Generating Function for the <span class="math inline">\(Geometric(p)\)</span> distribution is <span class="math display">\[\psi(t) = \frac{p}{1-(1-p)e^t}\]</span> <em>Hint, the Geometric distribution is named as such because the Geometric Series result is necessary to show this.</em></p></li>
<li><p>Utilize the mfg to derive the expected value and variance of a <span class="math inline">\(Geometric(p)\)</span> distribution.</p></li>
</ol>
<p>The Negative Binomial distribution extends the idea of “number of failures until the first success” to the number of failures until the <span class="math inline">\(r\)</span>th success.</p>
<ol start="3" style="list-style-type: decimal">
<li>The pf of the <span class="math inline">\(Y \sim Negative \;Binomial(r,p)\)</span> distribution can be derived with the following:
<ol style="list-style-type: lower-alpha">
<li>What is the probability of observing exactly <span class="math inline">\(y\)</span> failures in a row, followed by <span class="math inline">\(r\)</span> successes?</li>
<li>How many ways are there to distribute the <span class="math inline">\(r\)</span> successes among the <span class="math inline">\(r+y\)</span> draws, keeping in mind that the final draw must be a success?</li>
<li>Use the above ideas to derive the pf.</li>
</ol></li>
<li><p>A second way of thinking about the negative binomial distribution is as the sum of <span class="math inline">\(r\)</span> independent Geometric random variables. Utilize this interpretation to derive the expectation, variance, and moment generating function of a <span class="math inline">\(Negative \;Binomial(r,p)\)</span> distribution.</p></li>
<li><p>Suppose that we have a Negative Binomial distribution with expectation <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. What must the <span class="math inline">\(r\)</span> and <span class="math inline">\(p\)</span> be? <em>Notice this is just allows for an alternate paramterization of the distribution in terms of the mean and variance.</em></p></li>
<li>In the game <em>Pass the Pigs</em>, the probability of a Snouter is approximately 4%.
<ol style="list-style-type: lower-alpha">
<li>What is the expected number of failures before I roll my fourth Snouter?</li>
<li>What is the variance of the number of failures before I roll my fourth Snouter?</li>
</ol></li>
<li><p>Suppose that <span class="math inline">\(X\sim Geometric(p)\)</span>. What is the probability that <span class="math inline">\(X \in \{0,2,4,\dots\}\)</span>? <em>Hint: Let w=2x</em>.</p></li>
<li><p>Show that if <span class="math inline">\(X \sim Geometric(p)\)</span> that <span class="math inline">\(Pr( X \ge k ) = (1-p)^k\)</span> for <span class="math inline">\(k \in \{0,1,2,\dots\}\)</span>. _</p></li>
<li><p><strong>The Memoryless Property of the Geometric Distribution</strong> Let <span class="math inline">\(X\sim Geometric(p)\)</span>. Show that if it is known that <span class="math inline">\(X\ge k\)</span> for <span class="math inline">\(k \in \{0,1,2,\dots\}\)</span>, then <span class="math inline">\(Pr( X = k+t \;|\; X \ge k) = Pr( X =t )\)</span></p></li>
</ol>
<p><em>Some books parameterize the geometric (and negative binomial) distributions as the total number of draws before the first (or <span class="math inline">\(n\)</span>th) success. Whenever you are looking up properties of this distribution, make sure it is defined how you want it. For example, the wikipedia page for the geometric (and negative binomial) have the information for both definitions.</em></p>
</div>
<div id="normal" class="section level2">
<h2><span class="header-section-number">5.6</span> Normal</h2>
<p>The normal distribution plays a central role in statistics because there are many many asymptotic results that show that some quantity converges to a normal distribution.</p>
<p>The normal distribution is defined by it’s expectation <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> and has pdf <span class="math display">\[f(x) = \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ -\frac{(x-\mu)^2}{2\sigma^2} \right]\]</span> where <span class="math inline">\(\exp [w] = e^w\)</span> is just a notational convenience. Notice that we have no indicator function, and <span class="math inline">\(f(x) &gt; 0\)</span> for all <span class="math inline">\(x\in \mathbb{R}\)</span>.</p>
<p>One of the most tedious result to derive is that the pdf integrates to one (see the appendix) but can easily be done using the appropriate polar-coordinate transformation.</p>
<p>For our first result, we will derive the moment generating function.</p>
<p><span class="math display">\[\begin{aligned} \psi(t) 
  &amp;= E(e^{tX}) \\
  &amp;= \int_{-\infty}^\infty e^{tx} \; \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ -\frac{(x-\mu)^2}{2\sigma^2} \right] \,dx\\
  &amp;= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ tx -\frac{(x-\mu)^2}{2\sigma^2} \right] \,dx\\
  \end{aligned}\]</span> We then expand the square twice in the exponent by adding and subtracting the term <span class="math inline">\(\mu t + \frac{1}{2}\sigma^2 t^2\)</span> and then re-arranging to find</p>
<p><span class="math display">\[\begin{aligned} \psi(t) 
  &amp;= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ \mu t + \frac{1}{2}\sigma^2 t^2 -\mu t -\frac{1}{2} \sigma^2 t^2 + tx -\frac{(x-\mu)^2}{2\sigma^2}  \right] \,dx\\
  &amp;= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ \mu t + \frac{1}{2}\sigma^2 t^2 - \left\{ \mu t +\frac{1}{2} \sigma^2 t^2 - tx +\frac{(x-\mu)^2}{2\sigma^2}  \right\} \right] \,dx\\
  &amp;= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ \mu t + \frac{1}{2}\sigma^2 t^2 - \left\{\frac{2\sigma^2 \mu t + \sigma^4 t^2 - 2\sigma^2 tx + x^2 -2x\mu + \mu^2}{2\sigma^2} \right\} \right] \,dx\\
  &amp;= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ \mu t + \frac{1}{2}\sigma^2 t^2 - \left\{ \frac{x^2 - 2x(\mu+\sigma^2 t) + (\mu + \sigma^2t)^2}{ 2 \sigma^2} \right\} \right] \,dx\\
  &amp;= \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ \mu t + \frac{1}{2} \sigma^2 t^2 - \frac{\Big(x-(\mu+\sigma^2t)\Big)^2}{2\sigma^2} \right] \,dx\\
  &amp;= \exp \left[ \mu t + \frac{1}{2} \sigma^2 t^2 \right]  \int_{-\infty}^\infty \frac{1}{\sqrt{2\pi}\sigma} \exp \left[ - \frac{\Big(x-(\mu+\sigma^2t)\Big)^2}{2\sigma^2} \right] \,dx \\
  &amp;= \exp \left[ \mu t + \frac{1}{2} \sigma^2 t^2 \right]
  \end{aligned}\]</span></p>
<ol style="list-style-type: decimal">
<li><p>Show that if <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then <span class="math inline">\(E(X) = \mu\)</span>. <em>Hint, use the mgf.</em></p></li>
<li><p>Show that if <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then <span class="math inline">\(Var(X) = \sigma^2\)</span>. <em>Hint, use the mgf.</em></p></li>
<li><p>Show that if <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then <span class="math inline">\(Y=aX + b\)</span> has distribution <span class="math inline">\(Y\sim N(a\mu+b, \,a^2 \sigma^2)\)</span>. In particular, notice that <span class="math inline">\(Z=\frac{X-\mu}{\sigma}\)</span> has a <span class="math inline">\(N(0,1)\)</span> distribution.</p></li>
<li><p>Show that if <span class="math inline">\(X_i \sim N(\mu_i,\, \sigma^2_i)\)</span> for <span class="math inline">\(i \in \{1,2,\dots,n\}\)</span> where <span class="math inline">\(X_i\)</span> are all independent, then <span class="math inline">\(Y=\sum X_i\)</span> has distribution <span class="math inline">\(Y \sim N\left( \sum \mu_i, \sum \sigma^2_i \right)\)</span></p></li>
<li>In STA 270/275 it was claimed that the mean, median, and mode of a <span class="math inline">\(N(\mu,\sigma^2)\)</span> distribution are all <span class="math inline">\(\mu\)</span>. We can show this for the <span class="math inline">\(N(0,1)\)</span> case and due to the transformations confirmed in problem three, it will hold for all normal distributions. In the parts below, let <span class="math inline">\(Z\sim N(0,1)\)</span>
<ol style="list-style-type: lower-alpha">
<li>Use symmetric to show that the median of the standard normal distribution is 0.</li>
<li>Show that the peak of the distribution occurs at 0 using the first derivative test. The peak of a distribution is often called the <em>mode</em>.</li>
<li>Using the second derivative, show that the curve has inflection points at <span class="math inline">\(z=-1\)</span> and <span class="math inline">\(z=1\)</span> where the concavity changes.</li>
</ol></li>
<li>The amount of dry kibble that I feed my cats each morning can be well approximated by a normal distribution with mean <span class="math inline">\(\mu=200\)</span> grams and standard deviation <span class="math inline">\(\sigma=30\)</span> grams.
<ol style="list-style-type: lower-alpha">
<li>What is the probability that I fed my cats more than 250 grams of kibble this morning?</li>
<li>From my cats’ perspective, more food is better. How much would I have to feed them for this morning to be among the top <span class="math inline">\(10\%\)</span> of feedings?</li>
</ol></li>
</ol>
</div>
<div id="exponential-and-gamma" class="section level2">
<h2><span class="header-section-number">5.7</span> Exponential and Gamma</h2>
<ol style="list-style-type: decimal">
<li><p>Suppose that <span class="math inline">\(X \sim Gamma(\alpha, \beta)\)</span> and <span class="math inline">\(c\)</span> is a positive constant. Show that <span class="math inline">\(cX\)</span> is distributed <span class="math inline">\(Gamma(\alpha, \beta/c)\)</span>.</p></li>
<li><p>Find mode (peak of the distribution) for a <span class="math inline">\(Gamma(\alpha, \beta)\)</span> distribution.</p></li>
<li><p>Suppose that <span class="math inline">\(X_i \stackrel{iid}{\sim} Exp(\beta)\)</span> for <span class="math inline">\(i\in \{1,2,\dots,n\}\)</span>. Find the distribution of the sample mean <span class="math inline">\(\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(X_i \sim Exp(\beta_i)\)</span> are independently distributed for <span class="math inline">\(i\in \{1,2,\dots,n\}\)</span>. Show that the distribution of the sample minimum (<span class="math inline">\(Y = \min\{X_1, X_2, \dots, X_n\}\)</span> is <span class="math inline">\(Exp \left( \sum\beta_i \right)\)</span>.</p></li>
<li><p>Suppose that an electronic system contains <span class="math inline">\(n\)</span> similar components that function independently of each other and that are connected in series so that the system fails as soon as one of the components fails. Suppose also that the length of life for each component, measured in hours, has the exponential distribution with mean <span class="math inline">\(\mu\)</span>. Determine the mean and the variance of the length of time until the system fails.</p></li>
</ol>
</div>
<div id="beta" class="section level2">
<h2><span class="header-section-number">5.8</span> Beta</h2>
<ol style="list-style-type: decimal">
<li><p>Suppose that <span class="math inline">\(X \sim Beta(\alpha, \beta)\)</span>. Derive the expectation and variance of <span class="math inline">\(X\)</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(X \sim Beta(\alpha, \beta)\)</span>. Derive the distribution of <span class="math inline">\(Y = 1-X\)</span>.</p></li>
<li>Suppose that <span class="math inline">\(X \sim Gamma(\alpha_1, \beta)\)</span> and <span class="math inline">\(Y \sim Gamma(\alpha_2, \beta)\)</span> and that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent. Define <span class="math inline">\(U=X/(X+Y)\)</span> and <span class="math inline">\(V=X+Y\)</span>.
<ol style="list-style-type: lower-alpha">
<li>Derive the joint distribution of <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>. <em>Hint: This goes back to Section 3.9. Theorem 5.8.4 in this section is also helpful.</em></li>
<li>Show that <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are independent.</li>
<li>Show that <span class="math inline">\(U \sim Beta(\alpha_1, \alpha_2)\)</span></li>
</ol></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4-expectations.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="6-large-random-samples.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": false,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dereksonderegger/edit/master/474/05_Common_Distributions.Rmd",
"text": "Edit"
},
"download": [["Probability.pdf", "PDF"], ["Probability.epub", "EPUB"]],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
